{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://www.techsmith.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of RDD:  <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "RDD =sc.parallelize([\"Spark\", \"is\", \"a\", \"fr\", \"for\", \"BigData\"])\n",
    "\n",
    "print(\"The type of RDD: \", type(RDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of RDD1 is  <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "RDD1 = sc.textFile(\"5000_points.txt\", minPartitions = 4)\n",
    "print(\"The type of RDD1 is \", type(RDD1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'664159\\t550946'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type  <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "#File tu HDFS\n",
    "#RDD2 = sc.textFile(\"hdfs://SRVLT2:19000/t8.shakespeare.txt\")\n",
    "\n",
    "RDD2 = sc.textFile(\"hdfs://172.24.40.251:19000/t8.shakespeare.txt\")\n",
    "print(\"Type \", type(RDD2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD3 = sc.parallelize(RDD1.take(10))\n",
    "type(RDD3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'664159\\t550946'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD3.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 9, 25]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD4 = sc.parallelize([1,3,5])\n",
    "RDD_map = RDD4.map(lambda x : x * x)\n",
    "RDD_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_filter = RDD4.filter(lambda x : x > 3)\n",
    "RDD_filter.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data', 'Science', 'Machine', 'Learning', 'Big', 'Data']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatMap()\n",
    "\n",
    "RDD_string = sc.parallelize([\"Data Science\", \"Machine Learning\", \"Big Data\"])\n",
    "RDD_FlatMap = RDD_string.flatMap(lambda x: x.split(\" \"))\n",
    "RDD_FlatMap.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 6, 3, 7, 7, 3, 8]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD6 = sc.parallelize([1,3,5])\n",
    "RDD7= sc.parallelize([6,3,7])\n",
    "RDD8 = sc.parallelize([7,3,8])\n",
    "RDD_Union = RDD6.union(RDD7).union(RDD8)\n",
    "RDD_Union.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data', 'Science']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take\n",
    "RDD_FlatMap.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_FlatMap.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_FlatMap.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce: Tog hop cac phan tu\n",
    "RDD_reduce = RDD6.reduce(lambda  x, y : x + y)\n",
    "RDD_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD1.saveAsTextFile(\"5kPoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gom lai partition mong muon\n",
    "RDD1.coalesce(2).saveAsTextFile(\"5PointOpPartion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SV01', 'Tran A'), ('SV02', 'Tran B'), ('SV03', 'Tran C')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tuple = [(\"SV01\", \"Tran A\"), (\"SV02\", \"Tran B\"), (\"SV03\", \"Tran C\")]\n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "pairRDD_tuple.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'Science'), ('Machine', 'Learning'), ('Big', 'Data')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = [\"Data Science\", \"Machine Learning\", \"Big Data\"]\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "pair_RDD = regularRDD.map(lambda s: (s.split(\" \")[0], s.split(\" \")[1]))\n",
    "pair_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SV02 điểm tổng 3\n",
      "SV03 điểm tổng 1\n",
      "SV01 điểm tổng 7\n",
      "Sort By Key\n",
      "SV03 điểm tổng 1\n",
      "SV02 điểm tổng 3\n",
      "SV01 điểm tổng 7\n"
     ]
    }
   ],
   "source": [
    "#reduceByKey(): Ket hop cac value cung key\n",
    "RDD_source = sc.parallelize([(\"SV01\", 1), (\"SV01\", 2), (\"SV02\", 3), (\"SV01\", 4), (\"SV03\", 1)])\n",
    "RDD_reduceByKey = RDD_source.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "for num in RDD_reduceByKey.collect():\n",
    "    print(\"{} điểm tổng {}\".format(num[0], num[1]))\n",
    "    \n",
    "print(\"Sort By Key\")\n",
    "#sortByKey\n",
    "for num in RDD_reduceByKey.sortByKey(ascending=False).collect():\n",
    "    print(\"{} điểm tổng {}\".format(num[0], num[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SV02 [3]\n",
      "SV03 [1]\n",
      "SV01 [1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "#groupByKey\n",
    "RDD_group = RDD_source.groupByKey().collect()\n",
    "for code, marks in RDD_group:\n",
    "    print(code, list(marks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[80] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_source_2 = sc.parallelize([(\"SV01\", 1), (\"SV01\", 2), (\"SV02\", 3), (\"SV01\", 4), (\"SV04\", 4)])\n",
    "RDD_join = RDD_source.join(RDD_source_2)\n",
    "RDD_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SV02', (3, 3)),\n",
       " ('SV01', (1, 1)),\n",
       " ('SV01', (1, 2)),\n",
       " ('SV01', (1, 4)),\n",
       " ('SV01', (2, 1)),\n",
       " ('SV01', (2, 2)),\n",
       " ('SV01', (2, 4)),\n",
       " ('SV01', (4, 1)),\n",
       " ('SV01', (4, 2)),\n",
       " ('SV01', (4, 4))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_join.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join nối trên Key chung không có thành viên khác Key chung\n",
    "# miss SV03, SV04"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
